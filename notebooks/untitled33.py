# -*- coding: utf-8 -*-
"""Untitled33.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UV_nkFjW-4-Wookh_4k79o2vDXG8YYjj
"""

import pandas as pd

df = pd.read_csv('/content/WA_Fn-UseC_-Telco-Customer-Churn.csv')
df.head()

"""## Explore the data

### Subtask:
1. Display the first few rows of the DataFrame.
2. Check the data types of each column.
3. Look for any missing values in the dataset.
"""

df.head()

print('\t Data types for each columm')
df.dtypes

print('\t Columns with missing values')
df.isnull().sum()



"""## Analyze the target variable

### Subtask:
1. Examine the distribution of the 'Churn' variable.
2. Visualize the distribution of the 'Churn' variable.
"""

df['Churn'].value_counts()

import seaborn as sns
import matplotlib.pyplot as plt

plt.hist(df['Churn'])
plt.show()



"""## Analyze features

### Subtask:
1. Identify different types of features (numerical and categorical).
2. Analyze numerical features: explore their distribution using descriptive statistics and visualizations.
3. Analyze categorical features: explore their distribution and relationship with 'Churn' using value counts and visualizations.
"""

categorical_cols = df.select_dtypes(include=['object', 'category']).columns
numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns

print('Summary Statistics for numerical columns')
print(df[numerical_cols].describe())

plt.figure(figsize=(20, 10))
sns.heatmap(df[numerical_cols].corr(), annot = True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Heatmap for Numerical columns')
plt.show()

pip install bioinfokit

from bioinfokit.analys import stat
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

def test_associtation(df, x,y):
  table = pd.crosstab(df[x], df[y])
  test = stat()
  test.chisq(df= table)

  print('Null hypothesis: {} and {} are independent'.format(x,y))
  print('Alternative hypothesis: {} and {} are Dependent\n'.format(x,y))
  print('Test result')
  print('='*50)
  print('Observed frequency values \n')
  print(table)
  print()
  print('*' * 50)
  print(test.expected_df)
  print('*' * 50)
  print(test.summary)
  print('Visualizing {} and {}'.format(x,y))
  sns.countplot(x=x, hue=y, data=df)
  plt.title(f'Distribution of Churn by {x}')
  plt.show()


categorical_cols = df.select_dtypes(include=['object', 'category']).columns
numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns

# Exclude 'customerID' from categorical columns for association test
categorical_cols = categorical_cols.drop('customerID', errors='ignore')

for col in categorical_cols:
  if col == 'TotalCharges':
    continue
  test_associtation(df, col, 'Churn')

"""## Handle missing values

### Subtask:
1. Re-check for missing values in the DataFrame.
2. If any missing values are found, decide on an appropriate strategy to handle them.
3. Implement the chosen strategy to handle the missing values.
"""

df.isnull().sum()

df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')

df.isnull().sum()

df = df.dropna()

df.isnull().sum()

df.info()

"""## Handle missing values

### Subtask:
1. Drop rows with missing values.
2. Verify that there are no more missing values in the DataFrame.
"""

df = df.dropna()

df.info()



"""## Preprocess the data

### Subtask:
1. Encode categorical variables.
"""

print(categorical_cols)

df1 = pd.get_dummies(df, columns = ['gender', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines',
       'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection',
       'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract',
       'PaperlessBilling', 'PaymentMethod', 'Churn'], drop_first = True)

"""### Subtask:
2. Scale numerical features.
"""

df1.columns

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

# Select the numerical columns
numerical_cols = df1.select_dtypes(include=['int64', 'float64']).columns
numerical_data = df1[numerical_cols]

# Fit the scaler on the numerical data and transform it
scaled_numerical_data = scaler.fit_transform(numerical_data)

# Assign the scaled data back to the DataFrame
df1[numerical_cols] = scaled_numerical_data

# The customerID column was already handled during one-hot encoding, so this line is not needed.
# df1 = df1.drop('customerID')

df1.columns

df2 = df1[[ 'SeniorCitizen', 'tenure', 'MonthlyCharges',
       'TotalCharges', 'gender_Male', 'Partner_Yes', 'Dependents_Yes',
       'PhoneService_Yes', 'MultipleLines_No phone service',
       'MultipleLines_Yes', 'InternetService_Fiber optic',
       'InternetService_No', 'OnlineSecurity_No internet service',
       'OnlineSecurity_Yes', 'OnlineBackup_No internet service',
       'OnlineBackup_Yes', 'DeviceProtection_No internet service',
       'DeviceProtection_Yes', 'TechSupport_No internet service',
       'TechSupport_Yes', 'StreamingTV_No internet service', 'StreamingTV_Yes',
       'StreamingMovies_No internet service', 'StreamingMovies_Yes',
       'Contract_One year', 'Contract_Two year', 'PaperlessBilling_Yes',
       'PaymentMethod_Credit card (automatic)',
       'PaymentMethod_Electronic check', 'PaymentMethod_Mailed check',
       'Churn_Yes']]

df2.head()



"""## Split the data

### Subtask:
1. Separate features (X) and target variable (y).
2. Split data into training and testing sets.
"""

from sklearn.model_selection import train_test_split

X = df2.drop(columns=['Churn_Yes'])
y = df2['Churn_Yes']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42,stratify = y)

from sklearn.linear_model import LogisticRegression

LR = LogisticRegression()
LR.fit(X_train, y_train)

from sklearn.metrics import classification_report, roc_auc_score
y_pred = LR.predict(X_test)
y_pred_prob = LR.predict_proba(X_test)[:, 1]

report = classification_report(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_pred_prob)
print(report)
print(roc_auc)

"""### Subtask:
Train a Random Forest model.
"""

from sklearn.ensemble import RandomForestClassifier

rfc = RandomForestClassifier(n_estimators= 50, max_depth= 10)
rfc.fit(X_train, y_train)



"""### Subtask:
Evaluate the Random Forest model.
"""

rfc_pred = rfc.predict(X_test)
rfc_pred_prob = rfc.predict_proba(X_test)[:,1]
rfc_report = classification_report(y_test, rfc_pred)
rfc_roc = roc_auc_score(y_test, rfc_pred_prob)
print(rfc_report)
print(rfc_roc)

"""### Subtask:
Train and evaluate a Support Vector Machine (SVM) model.
"""

from sklearn.svm import SVC

svm = SVC(kernel = 'linear', C = 1.0, probability= True)
svm.fit(X_train, y_train)

svm_pred= svm.predict(X_test)
svm_pred_prob = svm.predict_proba(X_test)[:, 1]
svm_report = classification_report(y_test, svm_pred)
svm_roc = roc_auc_score(y_test, svm_pred_prob)
print(svm_report)
print(svm_roc)



"""## Model Interpretation

### Subtask:
Use SHAP to explain the model and identify feature importance.
"""

!pip install shap

import shap
import numpy as np

explainer = shap.LinearExplainer(LR, X_test)
shap_values = explainer.shap_values(X_test)

# Convert shap_values and X_test to NumPy arrays with float64 dtype for plotting compatibility
shap_values_display = np.array(shap_values).astype(np.float64)
X_test_display = X_test.values.astype(np.float64)

# Use the converted data for the summary plot
shap.summary_plot(shap_values_display, X_test_display, feature_names = X_test.columns)

